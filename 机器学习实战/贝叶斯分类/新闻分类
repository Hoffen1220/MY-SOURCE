# -*- coding:utf-8 -*-
# Author: Tianbao
import os
import jieba
import random
import matplotlib.pyplot as plt
from sklearn.naive_bayes import MultinomialNB


def textProcessing(folderPath, testSize=0.2):
    """
    中文文本处理
    :param folderPath: 文档路径
    :param testSize: 测试集占比
    :return:allWordList-按词频降序排序的训练集列表
            trainDataList-训练集列表
            testDataList-测试集列表
            trainClassList-训练集标签列表
            testClassList-测试集标签列表
    """
    folderList = os.listdir(folderPath)
    dataList = []
    classList = []

    for folder in folderList:
        newFolderPath = os.path.join(folderPath, folder)  # 根据子文件夹生成新路径
        files = os.listdir(newFolderPath)

        j = 1
        for file in files:
            if j > 100:
                break
            with open(os.path.join(newFolderPath, file), 'r', encoding='utf-8') as f:
                raw = f.read()

            wordCut = jieba.cut(raw, cut_all=False)
            wordList = list(wordCut)

            dataList.append(wordList)
            classList.append(folder)
            j += 1
    # print(dataList)
    # print(classList)

    dataClassList = list(zip(dataList, classList))  # zip压缩合并，将数据与标签对应压缩
    random.shuffle(dataClassList)  # 乱序
    index = int(len(dataClassList) * testSize) + 1
    trainList = dataClassList[index:]
    testList = dataClassList[:index]
    trainDataList, trainClassList = zip(*trainList)  # 训练集解压缩
    testDataList, testClassList = zip(*testList)

    allWordsDict = {}  # 统计训练集词频
    for wordList in trainDataList:
        for word in wordList:
            if word in allWordsDict.keys():
                allWordsDict[word] += 1
            else:
                allWordsDict[word] = 1

    # 根据键值倒序排序
    allWordsTupleList = sorted(allWordsDict.items(), key=lambda f: f[1], reverse=True)
    allWordsList, allWordsNum = zip(*allWordsTupleList)
    allWordsList = list(allWordsList)
    return allWordsList, trainDataList, testDataList, trainClassList, testClassList


def makeWordsSet(wordsFile):
    """
    读取文件内容并去重
    :param wordsFile:文件路径
    :return: wordsSet-读取的内容set集合
    """
    wordsSet = set()
    with open(wordsFile, 'r', encoding='utf-8') as f:
        for line in f.readlines():
            word = line.strip()  # 去回车
            if len(word) > 0:
                wordsSet.add(word)
    return wordsSet


def wordsDict(allWordsList, deleteN, stopwordsSet=set()):
    """
    文本特征选取
    :param allWordsList:训练及所有文本列表
    :param deleteN:删除词频最高的deleteN个词
    :param stopwordsSet:指定的结束语
    :return:featureWords-特征集
    """
    featureWords = []
    n = 1
    for t in range(deleteN, len(allWordsList), 1):
        if n > 1000:
            break
        # 如果这个词不是数字，并且不是指定的结束语，并且单词长度大于1小于5，那么这个词就可以作为特征词
        if not allWordsList[t].isdigit() and allWordsList[t] not in stopwordsSet and 1 < len(allWordsList[t]) < 5:
            featureWords.append(allWordsList[t])
        n += 1
    return featureWords


def textFeatures(trainDataList, testDataList, featureWords):
    """
    根据featureWords将文本向量化
    :param trainDataList:训练集
    :param testDataList:测试集
    :param featureWords:特征集
    :return:trainFeatureList-训练集向量化列表
            testFeatureList-测试集向量化列表
    """

    def text_features(text, featureWords):
        textWords = set(text)
        features = [1 if word in textWords else 0 for word in featureWords]
        return features

    trainFeatureList = [text_features(text, featureWords) for text in trainDataList]
    testFeatureList = [text_features(text, featureWords) for text in testDataList]
    return trainFeatureList, testFeatureList


def textClassfier(trainFeatureList, testFeatureList, trainClassList, testClassList):
    classfier = MultinomialNB().fit(trainFeatureList, trainClassList)
    testAccuracy = classfier.score(testFeatureList, testClassList)
    return testAccuracy


if __name__ == '__main__':
    # 文本预处理
    folderPath = './SogouC/Sample'
    allWordsList, trainDataList, testDataList, trainClassList, testClassList = textProcessing(folderPath)
    # print(allWordsList)
    # 生成stopwordsSet
    stopwordsFile = 'stopwords_cn.txt'
    stopwordsSet = makeWordsSet(stopwordsFile)

    testAccuracyList = []
    featureWords = wordsDict(allWordsList, 450, stopwordsSet)
    trainFeatureList, testFeatureList = textFeatures(trainDataList, testDataList, featureWords)
    testAccuracy = textClassfier(trainFeatureList, testFeatureList, trainClassList, testClassList)
    testAccuracyList.append(testAccuracy)
    ave = lambda c: sum(c) / len(c)
    print(ave(testAccuracyList))
    '''
    deleteNs = range(0, 1000, 20)
    for deleteN in deleteNs:
        featureWords = wordsDict(allWordsList, deleteN, stopwordsSet)
        # print(featureWords)
        trainFeatureList, testFeatureList = textFeatures(trainDataList, testDataList, featureWords)
        testAccuracy = textClassfier(trainFeatureList, testFeatureList, trainClassList, testClassList)
        testAccuracyList.append(testAccuracy)

    plt.figure()
    plt.plot(deleteNs, testAccuracyList)
    plt.title('Relationship of deleteNs and test_accuracy')
    plt.xlabel('deleteNs')
    plt.ylabel('test_accuracy')
    plt.show()
    '''
